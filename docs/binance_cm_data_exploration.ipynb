{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f357be95",
   "metadata": {},
   "source": [
    "# Binance Coin-Margined Futures Data Exploration\n",
    "\n",
    "This notebook demonstrates how to load gzip-compressed Binance Coin-Margined (CM) futures market data captured by the Rust collector into pandas for exploratory analysis. Each `.gz` file contains newline-delimited records with a nanosecond receive timestamp followed by the JSON payload emitted by the Binance websocket stream. We will parse these files efficiently and build a few exemplar visualizations to jump-start analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee570535",
   "metadata": {},
   "source": [
    "## 1. Environment setup\n",
    "\n",
    "We import the core Python libraries used for data manipulation and visualization.\n",
    "- **pandas**: tabular data handling\n",
    "- **numpy**: numerical helpers\n",
    "- **matplotlib**/**seaborn**: charting utilities\n",
    "- **pathlib**: convenient filesystem navigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531edb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure plotting aesthetics\n",
    "sns.set_theme(style='darkgrid', context='talk')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc48387",
   "metadata": {},
   "source": [
    "## 2. Locate available market data files\n",
    "\n",
    "The collector organises files under `/data/binance_cm/<symbol>/<symbol>_YYYYMMDD.gz`. The snippet below walks the directory tree, verifies availability, and previews a few gzip archives per symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561132aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('/data/binance_cm')\n",
    "\n",
    "if not DATA_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f'Data directory {DATA_DIR} not found. Ensure the path is mounted inside the environment before running the notebook.'\n",
    "    )\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "gz_files = sorted(DATA_DIR.rglob('*.gz'))\n",
    "print(f'Found {len(gz_files)} gzip file(s) under {DATA_DIR}.')\n",
    "\n",
    "files_by_symbol: dict[str, list[Path]] = defaultdict(list)\n",
    "for path in gz_files:\n",
    "    symbol = path.parent.name.upper()\n",
    "    files_by_symbol[symbol].append(path)\n",
    "\n",
    "preview = {symbol: paths[:3] for symbol, paths in files_by_symbol.items()}\n",
    "for symbol, paths in preview.items():\n",
    "    print(f\"Symbol: {symbol}\")\n",
    "    for path in paths:\n",
    "        print(f\"  - {path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b696b82",
   "metadata": {},
   "source": [
    "## 3. Helpers to parse collector output\n",
    "\n",
    "Collector files store each websocket event as `<recv_timestamp_ns> <json_payload>`. The helpers below stream gzip archives, optionally filter by event type (e.g. `trade`, `depthUpdate`), and assemble a clean, time-indexed `DataFrame`. Numeric fields delivered as strings (Binance trades encode price/size as strings) are converted to floats for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0296120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_collector_line(line: str) -> tuple[int, dict] | None:\n",
    "    \"\"\"Parse a single collector line into receive timestamp and payload dict.\"\"\"\n",
    "\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        ts_str, payload_json = line.split(' ', 1)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        timestamp_ns = int(ts_str)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        payload = json.loads(payload_json)\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "\n",
    "    if isinstance(payload, dict):\n",
    "        data = payload.get('data', payload)\n",
    "        if isinstance(data, dict):\n",
    "            return timestamp_ns, data\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def count_event_types(symbol: str, limit_messages: int | None = 100_000) -> pd.Series:\n",
    "    \"\"\"Return a Series with event-type counts for a symbol (sampled if requested).\"\"\"\n",
    "\n",
    "    symbol_dir = DATA_DIR / symbol.lower()\n",
    "    if not symbol_dir.exists():\n",
    "        raise FileNotFoundError(f'Could not find directory for symbol {symbol!r} under {DATA_DIR}.')\n",
    "\n",
    "    from collections import Counter\n",
    "\n",
    "    counter: Counter[str] = Counter()\n",
    "    messages_seen = 0\n",
    "\n",
    "    for path in sorted(symbol_dir.glob('*.gz')):\n",
    "        with gzip.open(path, 'rt') as fh:\n",
    "            for raw_line in fh:\n",
    "                parsed = parse_collector_line(raw_line)\n",
    "                if parsed is None:\n",
    "                    continue\n",
    "                _, data = parsed\n",
    "                event_type = str(data.get('e', 'unknown'))\n",
    "                counter[event_type] += 1\n",
    "                messages_seen += 1\n",
    "                if limit_messages is not None and messages_seen >= limit_messages:\n",
    "                    break\n",
    "        if limit_messages is not None and messages_seen >= limit_messages:\n",
    "            break\n",
    "\n",
    "    if not counter:\n",
    "        raise ValueError(f'No messages found for symbol {symbol!r}.')\n",
    "\n",
    "    return pd.Series(counter).sort_values(ascending=False)\n",
    "\n",
    "\n",
    "def load_symbol_events(\n",
    "    symbol: str,\n",
    "    event_type: str | None = 'trade',\n",
    "    limit_messages: int | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Load events for a symbol into a pandas DataFrame.\"\"\"\n",
    "\n",
    "    symbol_dir = DATA_DIR / symbol.lower()\n",
    "    if not symbol_dir.exists():\n",
    "        raise FileNotFoundError(f'Could not find directory for symbol {symbol!r} under {DATA_DIR}.')\n",
    "\n",
    "    files = sorted(symbol_dir.glob('*.gz'))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f'No gzip files found for symbol {symbol!r}.')\n",
    "\n",
    "    events: list[dict] = []\n",
    "    messages_loaded = 0\n",
    "\n",
    "    for path in files:\n",
    "        with gzip.open(path, 'rt') as fh:\n",
    "            for raw_line in fh:\n",
    "                parsed = parse_collector_line(raw_line)\n",
    "                if parsed is None:\n",
    "                    continue\n",
    "                recv_timestamp_ns, data = parsed\n",
    "                if event_type is not None and data.get('e') != event_type:\n",
    "                    continue\n",
    "\n",
    "                record = dict(data)\n",
    "                record['recv_timestamp_ns'] = recv_timestamp_ns\n",
    "                events.append(record)\n",
    "\n",
    "                messages_loaded += 1\n",
    "                if limit_messages is not None and messages_loaded >= limit_messages:\n",
    "                    break\n",
    "        if limit_messages is not None and messages_loaded >= limit_messages:\n",
    "            break\n",
    "\n",
    "    if not events:\n",
    "        raise ValueError(\n",
    "            f'No events matched symbol={symbol!r} event_type={event_type!r} with the provided limits.'\n",
    "        )\n",
    "\n",
    "    df = pd.DataFrame(events)\n",
    "\n",
    "    recv_timestamp_ns = df.pop('recv_timestamp_ns')\n",
    "    df.insert(0, 'recv_timestamp_ns', recv_timestamp_ns)\n",
    "\n",
    "    timestamp = pd.to_datetime(recv_timestamp_ns, unit='ns', utc=True)\n",
    "    df.index = pd.DatetimeIndex(timestamp, name='timestamp')\n",
    "\n",
    "    if 'E' in df.columns:\n",
    "        df.insert(1, 'event_time', pd.to_datetime(df['E'], unit='ms', utc=True, errors='coerce'))\n",
    "\n",
    "    for column in df.select_dtypes(include='object'):\n",
    "        df[column] = pd.to_numeric(df[column], errors='ignore')\n",
    "\n",
    "    if {'p', 'q'}.issubset(df.columns):\n",
    "        df['price'] = pd.to_numeric(df['p'], errors='coerce')\n",
    "        df['quantity'] = pd.to_numeric(df['q'], errors='coerce')\n",
    "        df['notional'] = df['price'] * df['quantity']\n",
    "\n",
    "    return df.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bd1501",
   "metadata": {},
   "source": [
    "## 4. Load a sample symbol\n",
    "\n",
    "Update `sample_symbol` to any symbol present in your dataset. We inspect the mix of event types captured (trades, depth updates, etc.) and then load trade prints into a tidy DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1e6d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_symbol = next(iter(files_by_symbol)) if files_by_symbol else None\n",
    "if sample_symbol is None:\n",
    "    raise RuntimeError('No symbols detected. Ensure the dataset directory contains gzip files.')\n",
    "\n",
    "print(f'Using sample symbol: {sample_symbol}')\n",
    "\n",
    "event_counts = count_event_types(sample_symbol, limit_messages=200_000)\n",
    "event_counts\n",
    "\n",
    "cm_trades = load_symbol_events(sample_symbol, event_type='trade', limit_messages=250_000)\n",
    "cm_trades.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd64295d",
   "metadata": {},
   "source": [
    "### Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d965988",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_trades.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9560147c",
   "metadata": {},
   "source": [
    "## 5. Resample to higher timeframes\n",
    "\n",
    "High-frequency data can be noisy. We aggregate to 1-minute candles (open, high, low, close) and traded volume to facilitate plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab019742",
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled = cm_trades[['price', 'quantity']].dropna()\n",
    "resampled = resampled.resample('1min').agg({\n",
    "    'price': ['first', 'max', 'min', 'last'],\n",
    "    'quantity': 'sum'\n",
    "}).dropna()\n",
    "\n",
    "# Flatten column MultiIndex\n",
    "resampled.columns = ['open', 'high', 'low', 'close', 'volume']\n",
    "resampled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e517b6e",
   "metadata": {},
   "source": [
    "## 6. Visualizations\n",
    "\n",
    "We plot both price action and traded volume over time. Adjust the resampling window or columns to suit your analysis needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a7a6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, sharex=True, figsize=(16, 10))\n",
    "\n",
    "resampled['close'].plot(ax=axes[0], color='dodgerblue')\n",
    "axes[0].set_title(f'{sample_symbol} Close Price (1-minute trades)')\n",
    "axes[0].set_ylabel('Price')\n",
    "\n",
    "resampled['volume'].plot(ax=axes[1], color='salmon')\n",
    "axes[1].set_title(f'{sample_symbol} Aggregated Quantity (1-minute)')\n",
    "axes[1].set_ylabel('Contracts')\n",
    "axes[1].set_xlabel('Timestamp (UTC)')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf6014e",
   "metadata": {},
   "source": [
    "### Rolling volatility\n",
    "\n",
    "Rolling volatility (standard deviation of returns) is a useful diagnostic for regime changes.\n",
    "Feel free to adjust the window size for your strategy horizon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8393bc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = resampled['close'].pct_change()\n",
    "rolling_vol = returns.rolling(window=30, min_periods=15).std() * np.sqrt(30)\n",
    "\n",
    "ax = rolling_vol.plot(color='mediumseagreen')\n",
    "ax.set_title(f'{sample_symbol} Rolling Volatility (30-minute window)')\n",
    "ax.set_ylabel('Volatility (Ïƒ)')\n",
    "ax.set_xlabel('Timestamp (UTC)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae1a901",
   "metadata": {},
   "source": [
    "## 7. Next steps\n",
    "\n",
    "- Compare multiple symbols by repeating the workflow for each directory.\n",
    "- Join with funding-rate or open-interest data to understand market structure.\n",
    "- Export aggregated DataFrames with `DataFrame.to_parquet()` for downstream backtesting pipelines.\n",
    "\n",
    "This notebook can serve as a starting point for deeper feature engineering and signal research on Binance Coin-Margined futures markets."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
